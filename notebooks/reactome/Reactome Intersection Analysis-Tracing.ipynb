{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection Analysis and Tracing\n",
    "Adapted from curli intersection analysis by Sebastian (July 07, 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition:\n",
    "Given a list of source nodes and a list of target nodes, find the potential most important nodes from sources to targets. \n",
    "\n",
    "#### Approaches:\n",
    "1. Run personalized pagerank using source nodes. The pagerank value is considered to be the probability that the node is influenced by the source nodes. \n",
    "2. Run personalized reverse pagerank using target nodes. The reverse pagerank value is considered to be the probability that the node influences the target nodes.\n",
    "3. Use a probability formula to get a intersection pagerank that represents the possibility that the node being influenced by the source nodes and also influences the target nodes.\n",
    "\n",
    "<img src=\"img/intersection.png\" width=\"400\" height=\"200\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to run Intersection Analysis\n",
    "1. Connect to arango database\n",
    "2. Find input nodes (source nodes and target nodes) in arango database\n",
    "3. Load the whole network graph from arango to memory and create a networkx graph. NetworkX is a python network library.\n",
    "4. Perform intersection analysis\n",
    "    - Run personalized pagerank algorithm using source nodes to get pagerank values for each nodes \n",
    "that the source nodes can reach (forward direction). Those are the nodes influenced by the source nodes\n",
    "    - Run personalized reverse pagerank using target nodes to get the reverce pagerank values for each \n",
    "nodes that can reach the target nodes\n",
    "    - Calculate the intersection pageranks based on source pageranks and target reverse pageranks\n",
    "    - Export the pagerank values into __excel file__\n",
    "5. The user analyzes the pagerank values (sorting, filtering etc), and select the rows that are interesting\n",
    "6. Create intersection traces for selected nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install lifelike_gds package if not already installed (e.g. running in Google Colab)\n",
    "import importlib\n",
    "\n",
    "if importlib.util.find_spec('lifelike_gds') is None:\n",
    "  !pip install git+https://github.com/SBRG/GDS-Public\n",
    "\n",
    "# provide the path to the notebook folder in the github repository in case the notebook is run in Google Colab\n",
    "github_path = 'SBRG/GDS-Public/main/notebooks/reactome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from pathlib import PurePosixPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dommas/.local/share/virtualenvs/GDS-Public-uKBS7cUg/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from lifelike_gds.arango_network.reactome import *\n",
    "from lifelike_gds.arango_network.radiate_trace import RadiateTrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = PurePosixPath('input')\n",
    "output_dir = PurePosixPath('output')\n",
    "os.makedirs(output_dir, 0o777, True)\n",
    "# gds database name\n",
    "arango_dbname = 'reactome'\n",
    "# gds database version, free text, that can be used to describe the graph\n",
    "db_version = 'reactome-arango-test-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to arango database.\n",
    "If use BioCyc databases (e.g. EcoCyc, HumanCyc), use Class BioCycDB.  \n",
    "If use Reactome database, use Class ReactomeDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set database uri, username and password. \n",
    "# dbname is the arango database name for the running arango instance. The default database name is 'arango'\n",
    "database = ReactomeDB(dbname=arango_dbname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Define functions to get source/target nodes\n",
    "read input file with ids (stId or dbId)\n",
    "read input file with reference ids (gene_id or chebi_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "read csv file to get list of reactome nodes\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "csv_filename:  the input file\n",
    "id_name: the property name in reactome db, e.g. stId, dbId\n",
    "id_column: the column name for the id property\n",
    "\"\"\" \n",
    "def get_nodes_by_identity_from_file(csv_filename, id_name, id_column, sep=','):\n",
    "    csv_file_path = input_dir / csv_filename\n",
    "    if os.path.isfile(csv_file_path):\n",
    "      csv_file_ref = csv_file_path\n",
    "    else:\n",
    "      # if does not exist localy, pull from github\n",
    "      csv_file_ref = f'https://raw.githubusercontent.com/{github_path}/{csv_file_path}'\n",
    "        \n",
    "    df = pd.read_csv(csv_file_ref, sep=sep, dtype='str')\n",
    "    ids = [n for n in df[id_column]]\n",
    "    nodes = database.get_nodes_by_attr(ids, id_name)\n",
    "    print('file_rows:', len(df), ', nodes matched:', len(nodes))\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def get_chemical_nodes_by_chebi(csv_filename, chebi_id_column, sep=','):\n",
    "    csv_file_path = input_dir / csv_filename\n",
    "    if os.path.isfile(csv_file_path):\n",
    "      csv_file_ref = csv_file_path\n",
    "    else:\n",
    "      # if does not exist localy, pull from github\n",
    "      csv_file_ref = f'https://raw.githubusercontent.com/{github_path}/{csv_file_path}'\n",
    "        \n",
    "    df = pd.read_csv(csv_file_ref, sep=sep, dtype='str')\n",
    "    ids = [n for n in df[chebi_id_column]]\n",
    "    nodes = database.get_entity_nodes_by_chebi_ids(ids)\n",
    "    print('file_rows:', len(df), ', nodes matched:', len(nodes))\n",
    "    return nodes\n",
    "\n",
    "def get_protein_nodes_by_gene_id(csv_filename, gene_id_column, sep=','):\n",
    "    csv_file_path = input_dir / csv_filename\n",
    "    if os.path.isfile(csv_file_path):\n",
    "      csv_file_ref = csv_file_path\n",
    "    else:\n",
    "      # if does not exist localy, pull from github\n",
    "      csv_file_ref = f'https://raw.githubusercontent.com/{github_path}/{csv_file_path}'\n",
    "        \n",
    "    df = pd.read_csv(csv_file_ref, sep=sep, dtype='str')\n",
    "    ids = [n for n in df[gene_id_column]]\n",
    "    nodes = database.get_entity_nodes_by_gene_ids(ids)\n",
    "    print('file_rows:', len(df), ', nodes matched:', len(nodes))\n",
    "    return nodes\n",
    "\n",
    "def get_reference_nodes_by_chebi(csv_filename, chebi_id_column, sep=','):\n",
    "    csv_file_path = input_dir / csv_filename\n",
    "    if os.path.isfile(csv_file_path):\n",
    "      csv_file_ref = csv_file_path\n",
    "    else:\n",
    "      # if does not exist localy, pull from github\n",
    "      csv_file_ref = f'https://raw.githubusercontent.com/{github_path}/{csv_file_path}'\n",
    "        \n",
    "    df = pd.read_csv(csv_file_ref, sep=sep, dtype='str')\n",
    "    ids = [n for n in df[chebi_id_column]]\n",
    "    nodes = database.get_reference_nodes_by_chebi_ids(ids)\n",
    "    print('file_rows:', len(df), ', nodes matched:', len(nodes))\n",
    "    return nodes\n",
    "\n",
    "def get_reference_nodes_by_gene_id(csv_filename, gene_id_column, sep=','):\n",
    "    csv_file_path = input_dir / csv_filename\n",
    "    if os.path.isfile(csv_file_path):\n",
    "      csv_file_ref = csv_file_path\n",
    "    else:\n",
    "      # if does not exist localy, pull from github\n",
    "      csv_file_ref = f'https://raw.githubusercontent.com/{github_path}/{csv_file_path}'\n",
    "        \n",
    "    df = pd.read_csv(csv_file_ref, sep=sep, dtype='str')\n",
    "    ids = [n for n in df[gene_id_column]]\n",
    "    nodes = database.get_reference_nodes_by_gene_ids(ids)\n",
    "    print('file_rows:', len(df), ', nodes matched:', len(nodes))\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Find input nodes (source and target nodes) in arango database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read updown genes as sources, and metabolites as targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources: updown genes\n",
    "updown_genes_file = 'updown.entrez'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "updown_genes_file_path = input_dir / updown_genes_file\n",
    "if os.path.isfile(updown_genes_file_path):\n",
    "  updown_genes_file_ref = updown_genes_file_path\n",
    "else:\n",
    "  # if does not exist localy, pull from github\n",
    "  updown_genes_file_ref = f'https://raw.githubusercontent.com/{github_path}/{updown_genes_file_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 92 gene_ids, matched to 119 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_rows: 92 , nodes matched: 119\n"
     ]
    }
   ],
   "source": [
    "#df1 = pd.read_csv(updown_genes_file_ref))\n",
    "#updown_genes = [n for n in df1['gene_id']]\n",
    "updown_nodes = get_protein_nodes_by_gene_id(updown_genes_file, gene_id_column='gene_id')\n",
    "#print(f\"updown genes: {len(updown_genes)}, nodes: {len(updown_nodes)}\")\n",
    "\n",
    "\n",
    "# targets: metabolites\n",
    "metabs_file = 'metabolite.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metabs_file_path = input_dir / metabs_file\n",
    "if os.path.isfile(metabs_file_path):\n",
    "  metabs_file_ref = metabs_file_path\n",
    "else:\n",
    "  # if does not exist localy, pull from github\n",
    "  metabs_file_ref = f'https://raw.githubusercontent.com/{github_path}/{metabs_file_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 23 chebi_ids, matched to 13 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_rows: 23 , nodes matched: 13\n"
     ]
    }
   ],
   "source": [
    "#df3 = pd.read_csv(metabs_file_ref)\n",
    "#metabs = [n for n in df3['chebi']]\n",
    "\n",
    "######to be changed to reactome mapping function (get from radiate script)\n",
    "metabs_nodes = get_chemical_nodes_by_chebi(metabs_file, chebi_id_column='chebi')\n",
    "#print(f\"metabolites: {len(metabs)}, nodes: {len(metabs_nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the whole network graph from arango to memory and create a networkx graph\n",
    "\n",
    "Create a RadiateTrace instance.  \n",
    "RadiateTrace is a subclass of TraceGraphNx.  TraceGraphNx has a property __graph__, that is a networkx graph. After the graph is created by using data from arango graph database, all the algorithms and traces can be run using the python networkx library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: load reactome graph\n",
      "INFO: MultiDirectedGraph with 71225 nodes and 112575 edges\n"
     ]
    }
   ],
   "source": [
    "tracegraph = RadiateTrace(Reactome(database))\n",
    "# set up output directory where the excel and graph files will write to\n",
    "tracegraph.datadir = output_dir\n",
    "# initiate tracegraph by loading graph data from arango\n",
    "tracegraph.init_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform intersection analysis\n",
    "Run intersection pagerank analysis and export values into excel file.\n",
    "\n",
    "The pagerank analysis is performed using networkx graph that contains a set of nodes and set of edges. \n",
    "\n",
    "#### Set node sets for sources and targets.  \n",
    "A node set is a list of node ids with a name and description.\n",
    "\n",
    "We set two source node sets and one target node set.  Then we will perform intersection analysis from each source node set to the target node set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set source and target node sets\n",
    "SOURCE_SET = 'updown_genes'\n",
    "TARGET_SET = 'metabolites'\n",
    "\n",
    "tracegraph.set_node_set_from_arango_nodes(updown_nodes, SOURCE_SET, 'updown_genes')\n",
    "tracegraph.set_node_set_from_arango_nodes(metabs_nodes, TARGET_SET, 'metabolites')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call export_intersection_pageranks\n",
    "The method export_intersection_pageranks() performs the following steps\n",
    "1. Run personalized pagerank using source nodes\n",
    "2. Run personalized reverse pagerank using target nodes\n",
    "3. Calculate intersection pagerank based on source pagerank and target reverse pagerank\n",
    "4. Write values into excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: set pagerank and num reach for updown_genes\n",
      "INFO: set pagerank and num reach for metabolites\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export intersection pagerank to file  output/Intersection_analysis_for_updown_genes_and_metabolites.xlsx\n"
     ]
    }
   ],
   "source": [
    "# keep a clean copy of graph\n",
    "tracegraph.graph = tracegraph.orig_graph.copy()\n",
    "\n",
    "filename = f\"Intersection_analysis_for_{SOURCE_SET}_and_{TARGET_SET}.xlsx\"\n",
    "tracegraph.export_intersection_pageranks(filename, SOURCE_SET, TARGET_SET, num_nodes=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze the pagerank output file (excel), and select interesting rows for further analysis\n",
    "\n",
    "#### Suggestion:   \n",
    "Add a column 'select' for selecting top pagerank nodes, and set any selected rows to 1, then save the file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create intersection traces for the selected rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read manually selected top ranked nodes from the previous generated pagerank excel file\n",
    "We will read the column 'select' to get the selected rows (for intersection pagerank selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_pagerank_select_file_path = input_dir / f\"Intersection_analysis_for_{SOURCE_SET}_and_{TARGET_SET}_select.xlsx\"\n",
    "if os.path.isfile(intersect_pagerank_select_file_path):\n",
    "  intersect_pagerank_select_file_ref = intersect_pagerank_select_file_path\n",
    "else:\n",
    "  # if does not exist localy, pull from github\n",
    "  intersect_pagerank_select_file_ref = f'https://raw.githubusercontent.com/{github_path}/{intersect_pagerank_select_file_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#adapted by Sebastian to use stId-->double-check with Robin\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintersect_pagerank_select_file_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mselect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselect\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m df\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GDS-Public-uKBS7cUg/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GDS-Public-uKBS7cUg/lib/python3.9/site-packages/pandas/io/excel/_base.py:364\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    363\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GDS-Public-uKBS7cUg/lib/python3.9/site-packages/pandas/io/excel/_base.py:1191\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1191\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1195\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1196\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1197\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1198\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GDS-Public-uKBS7cUg/lib/python3.9/site-packages/pandas/io/excel/_base.py:1070\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1068\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1070\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1072\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1073\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1074\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GDS-Public-uKBS7cUg/lib/python3.9/site-packages/pandas/io/common.py:609\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value for `encoding_errors` (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.python.org/3/library/codecs.html#error-handlers \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor valid values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    618\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[Buffer]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GDS-Public-uKBS7cUg/lib/python3.9/site-packages/pandas/io/common.py:312\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    311\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    313\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GDS-Public-uKBS7cUg/lib/python3.9/site-packages/pandas/io/common.py:212\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/urllib/request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/urllib/request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/urllib/request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/urllib/request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "#adapted by Sebastian to use stId-->double-check with Robin\n",
    "df = pd.read_excel(intersect_pagerank_select_file_ref, usecols=['stId', 'select'])\n",
    "df = df[df['select']==1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted by Sebastian to use stId-->double-check with Robin\n",
    "#also double-check if get_nodes_by_attr function is ok to use here\n",
    "selected_stIds = [id for id in df['stId']]\n",
    "print(selected_stIds)\n",
    "selected_nodes = database.get_nodes_by_attr(selected_stIds, 'stId')\n",
    "print('No. of selected nodes: ', len(selected_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run pageranks again using a clean copy of the original graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracegraph.graph = tracegraph.orig_graph.copy()\n",
    "\n",
    "pr = 'pagerank'\n",
    "rev_pr = 'rev_pagerank'\n",
    "tracegraph.set_pagerank(SOURCE_SET, pagerank_prop=pr)\n",
    "tracegraph.set_pagerank(TARGET_SET, pagerank_prop=rev_pr, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add traces from sources to the intersection node, and intersection node to targets\n",
    "We will add traces from source nodes to each selected intersection nodes, and traces from each selected intersection nodes to targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write traces in one file\n",
    "tracegraph.add_traces_from_sources_to_each_selected_nodes(selected_nodes, SOURCE_SET, weighted_prop=pr)\n",
    "tracegraph.add_traces_from_each_selected_nodes_to_targets(selected_nodes, TARGET_SET, weighted_prop=rev_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tracegraph.write_to_sankey_file(f\"Intersection_traces_from_{SOURCE_SET}_to_{TARGET_SET}.graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
